#!/usr/bin/env python3
"""
æ‰¹é‡å…³ç³»æŠ½å–è„šæœ¬
å¤„ç†åŒ…å«å¤šä¸ªtextå­—æ®µçš„JSONæ•°æ®

ä½¿ç”¨æ–¹æ³•:
1. åŸºæœ¬ä½¿ç”¨: py -u batch_relation_extractor.py
2. æŒ‡å®šæ•°æ®æ–‡ä»¶: py -u batch_relation_extractor.py my_data.json
3. æŒ‡å®šä¿å­˜é—´éš”: py -u batch_relation_extractor.py my_data.json 10
4. ä»ä¸­é—´ç»“æœæ¢å¤: py -u batch_relation_extractor.py my_data.json batch_results_partial_15_of_50.json

åŠŸèƒ½ç‰¹ç‚¹:
- æ¯å¤„ç†Nä¸ªæ–‡æœ¬è‡ªåŠ¨ä¿å­˜ä¸­é—´ç»“æœ
- æ”¯æŒä»ä¸­é—´ç»“æœæ¢å¤å¤„ç†
- å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦
- ç”Ÿæˆè¯¦ç»†çš„å¤„ç†æŠ¥å‘Š
"""

import asyncio
import json
import os
import sys
from typing import Dict, Any, List
from standalone_relation_extractor import StandaloneRelationExtractor

class BatchRelationExtractor:
    """æ‰¹é‡å…³ç³»æŠ½å–å™¨"""
    
    def __init__(self, api_key: str, api_type: str = "deepseek"):
        """åˆå§‹åŒ–æ‰¹é‡æŠ½å–å™¨"""
        self.extractor = StandaloneRelationExtractor(api_key, api_type)
        self.results = []
    
    async def process_single_text(self, text: str, index: int) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡æœ¬"""
        print(f"\nğŸ“ å¤„ç†ç¬¬ {index + 1} ä¸ªæ–‡æœ¬...")
        print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        print(f"æ–‡æœ¬é¢„è§ˆ: {text[:100]}...")
        
        try:
            result = await self.extractor.extract_and_describe_relations(text)
            result["text_index"] = index
            result["text_preview"] = text[:200] + "..." if len(text) > 200 else text
            
            if result["success"]:
                print(f"âœ… ç¬¬ {index + 1} ä¸ªæ–‡æœ¬å¤„ç†æˆåŠŸ")
                print(f"   å®ä½“æ•°é‡: {result['metadata']['total_entities']}")
                print(f"   å…³ç³»æ•°é‡: {result['metadata']['total_relations']}")
                print(f"   æè¿°æ•°é‡: {result['metadata']['descriptions_generated']}")
            else:
                print(f"âŒ ç¬¬ {index + 1} ä¸ªæ–‡æœ¬å¤„ç†å¤±è´¥: {result['error']}")
            
            return result
            
        except Exception as e:
            print(f"âŒ ç¬¬ {index + 1} ä¸ªæ–‡æœ¬å¤„ç†å¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": str(e),
                "text_index": index,
                "text_preview": text[:200] + "..." if len(text) > 200 else text
            }
    
    async def process_batch(self, data: List[Dict[str, str]], save_interval: int = 5) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ•°æ®"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(data)} ä¸ªæ–‡æœ¬...")
        print(f"ğŸ’¾ æ¯å¤„ç† {save_interval} ä¸ªæ–‡æœ¬ä¿å­˜ä¸€æ¬¡ç»“æœ")
        
        results = []
        success_count = 0
        total_entities = 0
        total_relations = 0
        total_descriptions = 0
        
        for i, item in enumerate(data):
            if "text" not in item:
                print(f"âš ï¸ ç¬¬ {i + 1} ä¸ªæ•°æ®é¡¹ç¼ºå°‘ 'text' å­—æ®µï¼Œè·³è¿‡")
                continue
            
            text = item["text"]
            if not text or not text.strip():
                print(f"âš ï¸ ç¬¬ {i + 1} ä¸ªæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            result = await self.process_single_text(text, i)
            results.append(result)
            
            if result["success"]:
                success_count += 1
                total_entities += result['metadata']['total_entities']
                total_relations += result['metadata']['total_relations']
                total_descriptions += result['metadata']['descriptions_generated']
            
            # æ¯å¤„ç†æŒ‡å®šæ•°é‡çš„æ–‡æœ¬å°±ä¿å­˜ä¸€æ¬¡
            if (i + 1) % save_interval == 0:
                print(f"\nğŸ’¾ å·²å¤„ç† {i + 1} ä¸ªæ–‡æœ¬ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
                temp_summary = {
                    "total_texts": len(data),
                    "processed_texts": len(results),
                    "success_count": success_count,
                    "failure_count": len(results) - success_count,
                    "total_entities": total_entities,
                    "total_relations": total_relations,
                    "total_descriptions": total_descriptions,
                    "success_rate": success_count / len(results) if results else 0,
                    "progress": f"{i + 1}/{len(data)}"
                }
                
                temp_result = {
                    "summary": temp_summary,
                    "results": results,
                    "is_partial": True
                }
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                temp_filename = f"batch_results_partial_{i + 1}_of_{len(data)}.json"
                self.save_results(temp_result, temp_filename)
                print(f"âœ… ä¸­é—´ç»“æœå·²ä¿å­˜åˆ° {temp_filename}")
        
        # ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
        summary = {
            "total_texts": len(data),
            "processed_texts": len(results),
            "success_count": success_count,
            "failure_count": len(results) - success_count,
            "total_entities": total_entities,
            "total_relations": total_relations,
            "total_descriptions": total_descriptions,
            "success_rate": success_count / len(results) if results else 0,
            "progress": f"{len(data)}/{len(data)}"
        }
        
        return {
            "summary": summary,
            "results": results,
            "is_partial": False
        }
    
    def save_results(self, batch_result: Dict[str, Any], output_file: str = "batch_relation_results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(batch_result, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    def load_partial_results(self, partial_file: str) -> tuple:
        """åŠ è½½éƒ¨åˆ†ç»“æœï¼Œç”¨äºæ¢å¤å¤„ç†"""
        try:
            with open(partial_file, "r", encoding="utf-8") as f:
                partial_data = json.load(f)
            
            if partial_data.get("is_partial", False):
                results = partial_data.get("results", [])
                processed_count = len(results)
                print(f"âœ… ä» {partial_file} åŠ è½½äº† {processed_count} ä¸ªå·²å¤„ç†çš„ç»“æœ")
                return results, processed_count
            else:
                print("âŒ è¯¥æ–‡ä»¶ä¸æ˜¯éƒ¨åˆ†ç»“æœæ–‡ä»¶")
                return [], 0
                
        except Exception as e:
            print(f"âŒ åŠ è½½éƒ¨åˆ†ç»“æœå¤±è´¥: {e}")
            return [], 0

    async def resume_processing(self, data: List[Dict[str, str]], partial_file: str, save_interval: int = 5) -> Dict[str, Any]:
        """ä»éƒ¨åˆ†ç»“æœæ¢å¤å¤„ç†"""
        print(f"ğŸ”„ ä»éƒ¨åˆ†ç»“æœæ¢å¤å¤„ç†...")
        
        # åŠ è½½å·²å¤„ç†çš„ç»“æœ
        existing_results, processed_count = self.load_partial_results(partial_file)
        
        if processed_count == 0:
            print("âŒ æ— æ³•æ¢å¤ï¼Œä»å¤´å¼€å§‹å¤„ç†")
            return await self.process_batch(data, save_interval)
        
        print(f"ğŸ“Š å·²å¤„ç†: {processed_count}/{len(data)} ä¸ªæ–‡æœ¬")
        
        # ç»§ç»­å¤„ç†å‰©ä½™æ–‡æœ¬
        results = existing_results.copy()
        success_count = sum(1 for r in results if r.get("success", False))
        total_entities = sum(r.get('metadata', {}).get('total_entities', 0) for r in results if r.get("success", False))
        total_relations = sum(r.get('metadata', {}).get('total_relations', 0) for r in results if r.get("success", False))
        total_descriptions = sum(r.get('metadata', {}).get('descriptions_generated', 0) for r in results if r.get("success", False))
        
        for i in range(processed_count, len(data)):
            item = data[i]
            if "text" not in item:
                print(f"âš ï¸ ç¬¬ {i + 1} ä¸ªæ•°æ®é¡¹ç¼ºå°‘ 'text' å­—æ®µï¼Œè·³è¿‡")
                continue
            
            text = item["text"]
            if not text or not text.strip():
                print(f"âš ï¸ ç¬¬ {i + 1} ä¸ªæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            result = await self.process_single_text(text, i)
            results.append(result)
            
            if result["success"]:
                success_count += 1
                total_entities += result['metadata']['total_entities']
                total_relations += result['metadata']['total_relations']
                total_descriptions += result['metadata']['descriptions_generated']
            
            # æ¯å¤„ç†æŒ‡å®šæ•°é‡çš„æ–‡æœ¬å°±ä¿å­˜ä¸€æ¬¡
            if (i + 1) % save_interval == 0:
                print(f"\nğŸ’¾ å·²å¤„ç† {i + 1} ä¸ªæ–‡æœ¬ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
                temp_summary = {
                    "total_texts": len(data),
                    "processed_texts": len(results),
                    "success_count": success_count,
                    "failure_count": len(results) - success_count,
                    "total_entities": total_entities,
                    "total_relations": total_relations,
                    "total_descriptions": total_descriptions,
                    "success_rate": success_count / len(results) if results else 0,
                    "progress": f"{i + 1}/{len(data)}"
                }
                
                temp_result = {
                    "summary": temp_summary,
                    "results": results,
                    "is_partial": True
                }
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                temp_filename = f"batch_results_partial_{i + 1}_of_{len(data)}.json"
                self.save_results(temp_result, temp_filename)
                print(f"âœ… ä¸­é—´ç»“æœå·²ä¿å­˜åˆ° {temp_filename}")
        
        # ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
        summary = {
            "total_texts": len(data),
            "processed_texts": len(results),
            "success_count": success_count,
            "failure_count": len(results) - success_count,
            "total_entities": total_entities,
            "total_relations": total_relations,
            "total_descriptions": total_descriptions,
            "success_rate": success_count / len(results) if results else 0,
            "progress": f"{len(data)}/{len(data)}"
        }
        
        return {
            "summary": summary,
            "results": results,
            "is_partial": False
        }

async def load_data_from_file(file_path: str) -> List[Dict[str, str]]:
    """ä»æ–‡ä»¶åŠ è½½æ•°æ®"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if isinstance(data, list):
            return data
        elif isinstance(data, dict) and "text" in data:
            return [data]
        else:
            print("âŒ æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸ºåŒ…å«textå­—æ®µçš„å¯¹è±¡åˆ—è¡¨")
            return []
            
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return []

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ‰¹é‡å…³ç³»æŠ½å–è„šæœ¬å¯åŠ¨...")
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    input_file = "input_data.json"  # é»˜è®¤æ–‡ä»¶å
    save_interval = 5  # é»˜è®¤æ¯5ä¸ªæ–‡æœ¬ä¿å­˜ä¸€æ¬¡
    resume_file = None  # æ¢å¤æ–‡ä»¶
    
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    if len(sys.argv) > 2:
        try:
            save_interval = int(sys.argv[2])
        except ValueError:
            # å¦‚æœç¬¬äºŒä¸ªå‚æ•°ä¸æ˜¯æ•°å­—ï¼Œå¯èƒ½æ˜¯æ¢å¤æ–‡ä»¶
            resume_file = sys.argv[2]
            save_interval = 5
    if len(sys.argv) > 3 and resume_file is None:
        try:
            save_interval = int(sys.argv[3])
        except ValueError:
            print("âš ï¸ ä¿å­˜é—´éš”å‚æ•°å¿…é¡»æ˜¯æ•°å­—ï¼Œä½¿ç”¨é»˜è®¤å€¼5")
    
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ’¾ ä¿å­˜é—´éš”: æ¯ {save_interval} ä¸ªæ–‡æœ¬ä¿å­˜ä¸€æ¬¡")
    if resume_file:
        print(f"ğŸ”„ æ¢å¤æ–‡ä»¶: {resume_file}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        print("ğŸ’¡ è¯·åˆ›å»ºåŒ…å«ä»¥ä¸‹æ ¼å¼çš„JSONæ–‡ä»¶ï¼š")
        print("""
[
    {"text": "æ¡ˆä¾‹æè¿°1"},
    {"text": "æ¡ˆä¾‹æè¿°2"},
    {"text": "æ¡ˆä¾‹æè¿°3"}
]
        """)
        return
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“‚ ä»æ–‡ä»¶ {input_file} åŠ è½½æ•°æ®...")
    data = await load_data_from_file(input_file)
    
    if not data:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ•°æ®é¡¹")
    
    # è®¾ç½®APIå¯†é’¥
    api_key = os.getenv("DEEPSEEK_API_KEY", "sk-5783900819b04bee9bc177f7db24edac")
    if not api_key:
        print("âŒ è¯·è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
        return
    
    print(f"âœ… APIå¯†é’¥å·²è®¾ç½®: {api_key[:10]}...")
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
    batch_processor = BatchRelationExtractor(api_key, api_type="deepseek")
    
    # æ‰¹é‡å¤„ç†
    if resume_file and os.path.exists(resume_file):
        print(f"ğŸ”„ ä» {resume_file} æ¢å¤å¤„ç†...")
        batch_result = await batch_processor.resume_processing(data, resume_file, save_interval)
    else:
        print("ğŸš€ å¼€å§‹æ–°çš„æ‰¹é‡å¤„ç†...")
        batch_result = await batch_processor.process_batch(data, save_interval)
    
    # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
    summary = batch_result["summary"]
    print(f"\n=== æ‰¹é‡å¤„ç†æ±‡æ€» ===")
    print(f"æ€»æ–‡æœ¬æ•°: {summary['total_texts']}")
    print(f"å¤„ç†æ–‡æœ¬æ•°: {summary['processed_texts']}")
    print(f"æˆåŠŸæ•°: {summary['success_count']}")
    print(f"å¤±è´¥æ•°: {summary['failure_count']}")
    print(f"æˆåŠŸç‡: {summary['success_rate']:.2%}")
    print(f"æ€»å®ä½“æ•°: {summary['total_entities']}")
    print(f"æ€»å…³ç³»æ•°: {summary['total_relations']}")
    print(f"æ€»æè¿°æ•°: {summary['total_descriptions']}")
    
    # ä¿å­˜ç»“æœ
    output_file = f"batch_results_{len(data)}_texts.json"
    batch_processor.save_results(batch_result, output_file)
    
    # æ˜¾ç¤ºéƒ¨åˆ†æˆåŠŸç»“æœ
    successful_results = [r for r in batch_result["results"] if r["success"]]
    if successful_results:
        print(f"\n=== æˆåŠŸå¤„ç†çš„æ–‡æœ¬ç¤ºä¾‹ ===")
        for i, result in enumerate(successful_results[:3], 1):
            print(f"{i}. æ–‡æœ¬ {result['text_index'] + 1}:")
            print(f"   å®ä½“: {result['metadata']['total_entities']} ä¸ª")
            print(f"   å…³ç³»: {result['metadata']['total_relations']} ä¸ª")
            print(f"   æè¿°: {result['metadata']['descriptions_generated']} ä¸ª")
            print()

if __name__ == "__main__":
    asyncio.run(main())
